{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "- $\\text{by Michael Nielsen}$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    \n",
    "    def __init__(self, sizes) -> None:\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
    "        respective layers of the network. For example; If the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neuron, the second layer 3 neuron,\n",
    "        and the third layer 1 neuron, the biases and the weights for \n",
    "        the network are initialized randomly, using a Gaussian \n",
    "        distribution with the mean 0, variance 1. Note that the first \n",
    "        layer is assume to be input layer, and by convention we won't\n",
    "        set any biases for those neurons, since biases are only ever\n",
    "        used in computing the outputs form later layer (hidden layer)\"\"\"\n",
    "        \n",
    "        self.num_layer  = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x,y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "\n",
    "    def sigmoid(self, z) -> np.float32:\n",
    "        \"\"\"Sigmoid function\"\"\"\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "\n",
    "    def derivative_sigmoid(self, z) -> np.float32:\n",
    "        \"\"\"Derivative sigmoid function\"\"\"\n",
    "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
    "    \n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = self.sigmoid(np.dot(w,a) + b)\n",
    "\n",
    "        return a\n",
    "    \n",
    "\n",
    "    def loss(self, output_activation, y):\n",
    "        \"\"\"Return the vector of loss between output and y\"\"\"\n",
    "        return (output_activation - y)\n",
    "    \n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y) for x,y in test_data]\n",
    "\n",
    "        return sum(int(x == y) for (x,y) in test_results)\n",
    "\n",
    "    \n",
    "\n",
    "    def stochastic_gradient_descent(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        \"\"\"Train the neural net using mini_batch stochastic\n",
    "        gradient descent, The ``training_data`` is a list of \n",
    "        tuples ``(x, y)`` representing the training input and\n",
    "        the desired outputs, The other non-optional parameters\n",
    "        are self-explanatory. If ``test_data`` is provided then\n",
    "        # if type(value) is list:\n",
    "            # for val in value:\n",
    "                # print(val, '\\n')       the network will be evaluated against the test data after\n",
    "        each epoch, and partial progress printed out. This is \n",
    "        useful for tracking progress, but slows things down \n",
    "        substantially\"\"\"\n",
    "        if test_data:\n",
    "            n_test = len(test_data)\n",
    "\n",
    "        n = len(training_data)\n",
    "        for i in range(epochs):\n",
    "\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k: k + mini_batch_size] for k in range(0, n, mini_batch_size)\n",
    "            ]\n",
    "\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "\n",
    "            if test_data:\n",
    "                print(f\"Epoch {i}: {self.evaluate(test_data)} / {n_test} \")\n",
    "            else:\n",
    "                print(f\"Epoch {i} completed\")\n",
    "\n",
    "    \n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using back-propagation to a single \n",
    "        mini-batch. The ``mini_batch`` is a list of tuple ``(X, y)``\n",
    "        and eta is the learning rate or might refer to alpha\"\"\"\n",
    "\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "        for x, y in mini_batch:\n",
    "\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "\n",
    "            self.weights = [w - (eta/len(mini_batch)) * nw for w, nw in zip(self.weights, nabla_w)]\n",
    "            self.biases  = [b - (eta/len(mini_batch)) + nb for b, nb in zip(self.biases,  nabla_b)]\n",
    "\n",
    "        \n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the \n",
    "        gradient for the cost function C_x, ``nabla_b`` and ``nabla_w``\n",
    "        are layer-by-layer lists of numpy array, similar to\n",
    "        ``self.biases`` and ``self.weights``.\"\"\"\n",
    "\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x]\n",
    "        zs = []\n",
    "\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = self.sigmoid(z)\n",
    "            activations.append(activation)\n",
    "\n",
    "        # backward pass\n",
    "        delta = self.loss(activations[-1], y) * self.derivative_sigmoid(zs[-1])\n",
    "\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "                             \n",
    "        for l in range(2, self.num_layer):\n",
    "\n",
    "            z = zs[-l]\n",
    "            sp = self.derivative_sigmoid(z)\n",
    "            delta = np.dot(self.weights[-l + 1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l - 1].transpose())\n",
    "\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Testing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net Analysis\n",
    "\n",
    "- *Neural nets are now under my control*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Network*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural net params:\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \n",
      "\n",
      "sizes : [2, 3, 4, 1]\n",
      "\n",
      "n_layers : 4\n",
      "\n",
      "biases : [array([[-1.32765863],\n",
      "       [-0.93819124],\n",
      "       [-0.83962722]]), array([[-0.35147417],\n",
      "       [-0.37127407],\n",
      "       [ 0.21860779],\n",
      "       [ 0.57585532]]), array([[-1.39756237]])]\n",
      "\n",
      "weights : [array([[-0.93730449,  0.99891713],\n",
      "       [ 1.04260173, -0.59196504],\n",
      "       [-0.38881551, -1.21290153]]), array([[ 0.17229952,  0.28093324,  0.68613271],\n",
      "       [ 0.45046864, -0.03548488,  0.91363497],\n",
      "       [-0.32017231, -0.40024135,  2.53995249],\n",
      "       [-0.77135256,  1.24731747, -1.11937558]]), array([[-0.13590568,  0.78861676, -1.14907576,  0.85617697]])]\n",
      "\n",
      "biases_shape : [(3, 1), (4, 1), (1, 1)]\n",
      "\n",
      "weights_shape : [(3, 2), (4, 3), (1, 4)]\n",
      "\n",
      "\n",
      " ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sizes = [2,3,4,1]\n",
    "\n",
    "network = {'sizes':None, \n",
    "           'n_layers':None, \n",
    "           'biases' :None, \n",
    "           'weights':None,\n",
    "           'biases_shape' : None,\n",
    "           'weights_shape': None }\n",
    "\n",
    "def initialize_network(network_size):\n",
    "    \n",
    "    network['sizes'] = network_size\n",
    "    network['n_layers'] = len(network_size)\n",
    "    network['biases'] = [np.random.randn(n_neurons, 1) for n_neurons in sizes[1:]]\n",
    "    network['weights']  = [np.random.randn(input_size, n_neurons) for input_size, n_neurons  in zip(sizes[1:], sizes[:-1])]\n",
    "    network['biases_shape'] =  [bias.shape for bias in network['biases']]\n",
    "    network['weights_shape'] = [weight.shape for weight in network[\"weights\"]]\n",
    "\n",
    "\n",
    "def network_params():\n",
    "\n",
    "    print(\"Neural net params:\")\n",
    "    print(\"~\" * 60, '\\n')\n",
    "\n",
    "    for key, value in network.items():\n",
    "        print(f\"{key} : {value}\\n\")\n",
    "\n",
    "    print(\"\\n\", \"~\" * 60, '\\n')\n",
    "\n",
    "initialize_network(network_size=sizes)\n",
    "network_params()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Testing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[0.],\n",
       "         [0.],\n",
       "         [0.]]),\n",
       "  array([[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]),\n",
       "  array([[0.]])],\n",
       " '^',\n",
       " [array([[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]]),\n",
       "  array([[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]),\n",
       "  array([[0., 0., 0., 0.]])])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_biases  = [np.zeros(b.shape) for b in network['biases']]                      \n",
    "gradient_weights = [np.zeros(w.shape) for w in network['weights']] \n",
    "\n",
    "gradient_biases,\"^\" ,gradient_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *ReLU function*\n",
    "\n",
    "$\\text{Rectified Linear Unit}$\n",
    "\n",
    "- $\\large ReLU(x) = max(0, x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x) -> np.float32:\n",
    "    \"\"\"Rectified Linear Unit function\"\"\"\n",
    "    return np.max(0, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Sigmoid function*\n",
    "\n",
    "- $\\Large \\sigma(x) = \\frac{1}{1 + e^{-x}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x) -> np.float32:\n",
    "    \"\"\"Sigmoid function\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Derivative sigmoid function*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Large \\sigma^\\prime(x)  = \\frac{1}{1 + e^{-x}} \\frac{d\\sigma}{dx} = \\sigma(x)(1 - \\sigma(x))$\n",
    "\n",
    "$\\!$\n",
    "\n",
    "<details>\n",
    "    <summary><i>Derive sigmoid function</i></summary>\n",
    "    \n",
    "$\\textsf{Let’s derive this derivative of sigmoid function as follows:}$\n",
    "\n",
    "1. $\\textsf{Let} \\; \\large y = \\sigma(x) = \\large \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "2. $\\textsf{Let u = } \\; \\large{1 + e^{-x}.} \\;\\small \\textsf{Thus}, \\;\\large y = \\frac{1}{u}$ \n",
    "\n",
    "3. $\\textsf{First, find the derivative of u with respect to} \\; \\large x: \\frac{du}{dx} = -e^{-x}$\n",
    "\n",
    "4. $\\textsf{Then, find the derivative of y with respect to} \\; \\large u: \\frac{dy}{du} = -\\frac{1}{u^2}$\n",
    "\n",
    "5. $\\textsf{Apply the chain rule} \\; \\large : \\\\ \\quad \\Rightarrow \\frac{dy}{dx} =  \\frac{dy}{du} . \\frac{du}{dx} = -\\frac{1}{u^2} . (-e^{-x}) = \\frac{e^{-x}}{({1 + e^{-x}})^2}  $\n",
    "\n",
    "$\\large \\qquad \\; \\; \\Rightarrow \\sigma(x) = \\frac{1}{1 + e^{-x}} ,\\; 1 - \\sigma(x) = \\frac{e^{-x}}{1 + e^{-x}}$\n",
    "\n",
    "6. $\\textsf{Thus} ,\\; \\large \\sigma^\\prime(x) = \\frac{e^{-x}}{({1 + e^{-x}})^2} = (\\frac{1}{1 + e^{-x}})(\\frac{e^{-x}}{1 + e^{-x}}) = \\Large \\sigma(x)(1 - \\sigma(x))$\n",
    "\n",
    "*[Derivative Sigmoid function](https://www.geeksforgeeks.org/derivative-of-the-sigmoid-function/)*\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_sigmoid(z) -> np.float32:\n",
    "    \"\"\"Derivative sigmoid function\"\"\"\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Feed_forward*\n",
    "\n",
    "- $\\large z = w . a + b$\n",
    "\n",
    "- $\\large a = \\sigma(z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(a):                                               # -> a: as a input of the network\n",
    "    \"\"\"Return the output of the network by feed forward\n",
    "    a as a input through network layers\n",
    "    This network using sigmoid function for all neurons\"\"\"\n",
    "    for b, w in zip(network['biases'], network['weights']):       # -> Iterated through biases and weights simultaneously\n",
    "    \n",
    "        z = np.dot(w, a) + b                                      # -> Calculated weighted sum, by matrix multiplication\n",
    "        a = sigmoid(z)                                            # -> Pass through sigmoid function to learn non-linearity patter\n",
    "\n",
    "    return a                                                      # -> return a as a output of the network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Loss function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(output_activation, y):\n",
    "    \"\"\"Return the vector of loss between output and y\"\"\"\n",
    "    return (output_activation - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Loss function*\n",
    "\n",
    "$\\text{MSE loss with ReLU Activation}$\n",
    "\n",
    "- $\\Large C(y, w, X, b) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - max(0, w \\cdot X_i + b))^2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_ReLU(y, w, X, b, n):\n",
    "    \"\"\"Mean Square Error loss with ReLU Activation\"\"\"\n",
    "    return (1/n) * np.sum((y - np.max(0, w * X + b))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Back_propagation*\n",
    "\n",
    "$\\text{Backpropagation}:$\n",
    "- *This algorithm is used to efficiently compute the gradients of the loss function with respect to the weights and biases.*\n",
    "\n",
    "- *The backward pass calculates the gradients by starting from the output layer and propagating them back through the network*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagate(x, y):\n",
    "\n",
    "    gradient_biases  = [np.zeros(b.shape) for b in network['biases']]                 # *-> Initialize the gradient for biases as lists of arrays of zeros\n",
    "    gradient_weights = [np.zeros(w.shape) for w in network['weights']]                # *-> These two lists will store the calculated gradient for each bias and weight in the network\n",
    "\n",
    "    # feedforward\n",
    "    activation = x                           # -> The input x is stored as the initial activation\n",
    "    activations = [x]                        # -> The list activations stores the activation values at each layer, including the input layer\n",
    "    zs = []                                  # -> zs stores the weighted sum before applying activation function at each layer\n",
    "\n",
    "    for b, w in zip(network['biases'], network['weights']):         \n",
    "\n",
    "        z = np.dot(w, activation) + b               # -> the weighted sum(z) is calculated using current weight, activation func and bias\n",
    "        zs.append(z)                                # -> Store the weighted sum(z) (not applying activation function)\n",
    "        activation = sigmoid(z)                     # -> calculate and reassign the activation by applying z with sigmoid function( activation function)\n",
    "        activations.append(activation)              # -> Store the activation value \n",
    "\n",
    "    # backward pass                                                                         # !>> delta is called the error term\n",
    "    delta = loss(activations[-1], y)    *   derivative_sigmoid(zs[-1])                      # !-> delta is calculated as the derivative of the loss function with respect \n",
    "    #       loss(prediction, ground_truth)  zs[-1] is (weighted_sum of output layer)        # !-> to the output activations( using the loss and derivative_sigmoid function)\n",
    "    #       loss of binary class, sigmoid(a) = 0 or 1. corresponding to the class label.\n",
    "\n",
    "    #                                                                 # !>> The gradient for the output layer's bias and weights are calculated directly\n",
    "    gradient_biases[-1]  = delta                                      # !-> gra_biases[-1] is set to delta (the gradient for the output layer's bias)\n",
    "    gradient_weights[-1] = np.dot(delta, activations[-2].T)           # !-> gra_weight[-1] is calculated as the dot product of delta and the transposed\n",
    "    #                                                                 # !-> activation from the previous layer (the gradient for the output layer's weight)\n",
    "                            \n",
    "    for idx_layer in range(2, network[\"n_layers\"]):                       # >> Iterating through hidden layers (excluding output layer)\n",
    "    #                                                                     # -> Loop starting from (2, n_layers) but negatively indexing -> -2,-3,-4,-5,...{n_layers - 1}\n",
    "    \n",
    "        #                                                                                             # !>>  \n",
    "        z = zs[-idx_layer]                                                                            # !-> Retrieve the weighted sum(z) of the last second layer \n",
    "        delta = np.dot(delta, network['weights'][-idx_layer + 1].T ) * derivative_sigmoid(z)          # !-> Calculate the error term (delta) for current layer.\n",
    "        #                                                                                             # !-> Calculated the derivative of sigmoid function evaluated at z\n",
    "\n",
    "        gradient_biases[-idx_layer]  = delta                                                     # -> Update the gradient for the bias in the current layer\n",
    "        gradient_weights[-idx_layer] = np.dot(delta, activations[-idx_layer - 1].T)              # -> Update the gradient for the weight in cur layer.\n",
    "\n",
    "    return (gradient_biases,         # *-> gra_biases : A list containing the calculated gradients for each bias in the network, (layer by layer)\n",
    "            gradient_weights)        # *-> gra_weights: A list containing the calculated gradients for each weight in the network (layer by layer)\n",
    "\n",
    "\n",
    "# Note:\n",
    "\n",
    "# delta = np.dot(network['weights'][-l + 1].transpose(), delta) * sp:\n",
    "\n",
    "    # This line calculates the error term (delta) for the current layer. It involves several steps:\n",
    "        # network['weights'][-l + 1].transpose() : Transposes the weight matrix of the next layer (l + 1).\n",
    "        # np.dot(...): Performs the dot product between the transposed weight matrix and the previous error term (delta). This essentially propagates the error from the next layer back to the current layer.\n",
    "        # sp: Multiplies the propagated error by the activation derivative (sp). This takes into account how sensitive the activations in the current layer are to changes in the weighted sum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Update mini batch*\n",
    "\n",
    "$\\text{Gradient descent update rule}:$\n",
    "\n",
    "- $weight = weight - eta * gradient$\n",
    "\n",
    "- $bias = bias - eta * gradient$\n",
    "\n",
    "$\\text{Stochastic Gradient descent update rule}:$\n",
    "\n",
    "- $weight = weight - \\frac{eta}{batch\\_size} * accumulated\\_gradient$\n",
    "\n",
    "- $bias = bias - \\frac{eta}{batch\\_size} * accumulated\\_gradient$\n",
    "\n",
    "$\\text{Mini-Batches}:$\n",
    "- *By using mini-batches, SGD reduces the variance in the gradient estimates and can lead to faster convergence.*\n",
    "\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_mini_batch(mini_batch, eta, mini_batch_size):\n",
    "\n",
    "    gra_batch_biases  = [np.zeros(b.shape) for b in network['biases']]                        # *-> Batch gradient initialized as lists of arrays of zeros\n",
    "    gra_batch_weights = [np.zeros(w.shape) for w in network['weights']]                       # *-> Do this for initialization of weights and bias\n",
    "\n",
    "    for (x, y) in mini_batch:                                                                       # !-> Iterating each sample over mini_batch:\n",
    "\n",
    "        delta_gra_biases, delta_gra_weights = back_propagate(x, y)                                  # !-> back_propagating to calculate gradient for this one sample (each by each)\n",
    "        gra_batch_biases  = [gb + dgb for gb, dgb in zip(gra_batch_biases,  delta_gra_biases)]      # !-> Accumulate delta gradient by adding delta_gradient_bias to gradient_biases\n",
    "        gra_batch_weights = [gw + dgw for gw, dgw in zip(gra_batch_weights, delta_gra_weights)]     # !-> Do the same with weights, accumulated delta gradient\n",
    "\n",
    "    network['biases']  = [b - (eta/mini_batch_size) * gb for b, gb in zip(network['biases'],  gra_batch_biases)]    # -> Update weighs and biases after processing entire mini_batch\n",
    "    network['weights'] = [w - (eta/mini_batch_size) * gw for w, gw in zip(network['weights'], gra_batch_weights)]   # -> Scaled average gradients update by multiply with (eta/mini_batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Evaluate*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_data):\n",
    "    \n",
    "    test_results = [(np.argmax(feed_forward(x)), y) for x,y in test_data]\n",
    "    return sum(int(x == y) for (x,y) in test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Stochastic Gradient Descent*\n",
    "\n",
    "$\\text{Stochastic Gradient Descent}:$\n",
    "- *It's a variant of gradient descent that updates the model parameters using the gradient computed from a small random subset of the training data, called a mini-batch.*\n",
    "\n",
    "$\\text{Learning Rate}:$\n",
    "- *The learning rate eta controls the step size of the updates. A smaller learning rate can lead to more accurate results but slower convergence*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "\n",
    "    if test_data:                                           # -> Check for test set if included                         \n",
    "        n_test = len(test_data)\n",
    "\n",
    "    n = len(training_data)                                  # -> take the number of training samples\n",
    "    for epoch in range(epochs):                             # -> Training loop, iterate over epochs\n",
    "\n",
    "        random.shuffle(training_data)                       # !-> shuffle training data for each epoch => avoid local minimal\n",
    "        mini_batches = [\n",
    "            training_data[k: k + mini_batch_size]           # !-> k is the interval when looping \n",
    "            for k in range(0, n, mini_batch_size)           # !-> using k to slice the training_data into mini_batches\n",
    "        ]\n",
    "\n",
    "        for mini_batch in mini_batches:                                     # !-> Update weights and biases for each mini_batch\n",
    "            update_mini_batch(mini_batch, eta, mini_batch_size)             # !-> Calculated gradient for each mini_batch\n",
    "\n",
    "        if test_data:\n",
    "            print(f\"Epoch {epoch}: {evaluate(test_data)} / {n_test} \")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch} completed\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Hands-on testing\n",
    "\n",
    "\n",
    "### *Diabetes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019907</td>\n",
       "      <td>-0.017646</td>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068332</td>\n",
       "      <td>-0.092204</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005670</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>-0.025930</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022688</td>\n",
       "      <td>-0.009362</td>\n",
       "      <td>206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031988</td>\n",
       "      <td>-0.046641</td>\n",
       "      <td>135.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.019662</td>\n",
       "      <td>0.059744</td>\n",
       "      <td>-0.005697</td>\n",
       "      <td>-0.002566</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.031193</td>\n",
       "      <td>0.007207</td>\n",
       "      <td>178.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>-0.005515</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>-0.067642</td>\n",
       "      <td>0.049341</td>\n",
       "      <td>0.079165</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>-0.018114</td>\n",
       "      <td>0.044485</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>0.017293</td>\n",
       "      <td>-0.037344</td>\n",
       "      <td>-0.013840</td>\n",
       "      <td>-0.024993</td>\n",
       "      <td>-0.011080</td>\n",
       "      <td>-0.046883</td>\n",
       "      <td>0.015491</td>\n",
       "      <td>132.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.016318</td>\n",
       "      <td>0.015283</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.026560</td>\n",
       "      <td>0.044529</td>\n",
       "      <td>-0.025930</td>\n",
       "      <td>220.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.073030</td>\n",
       "      <td>-0.081413</td>\n",
       "      <td>0.083740</td>\n",
       "      <td>0.027809</td>\n",
       "      <td>0.173816</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.004222</td>\n",
       "      <td>0.003064</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>442 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0    0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1   -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2    0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356   \n",
       "3   -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4    0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "437  0.041708  0.050680  0.019662  0.059744 -0.005697 -0.002566 -0.028674   \n",
       "438 -0.005515  0.050680 -0.015906 -0.067642  0.049341  0.079165 -0.028674   \n",
       "439  0.041708  0.050680 -0.015906  0.017293 -0.037344 -0.013840 -0.024993   \n",
       "440 -0.045472 -0.044642  0.039062  0.001215  0.016318  0.015283 -0.028674   \n",
       "441 -0.045472 -0.044642 -0.073030 -0.081413  0.083740  0.027809  0.173816   \n",
       "\n",
       "           s4        s5        s6  target  \n",
       "0   -0.002592  0.019907 -0.017646   151.0  \n",
       "1   -0.039493 -0.068332 -0.092204    75.0  \n",
       "2   -0.002592  0.002861 -0.025930   141.0  \n",
       "3    0.034309  0.022688 -0.009362   206.0  \n",
       "4   -0.002592 -0.031988 -0.046641   135.0  \n",
       "..        ...       ...       ...     ...  \n",
       "437 -0.002592  0.031193  0.007207   178.0  \n",
       "438  0.034309 -0.018114  0.044485   104.0  \n",
       "439 -0.011080 -0.046883  0.015491   132.0  \n",
       "440  0.026560  0.044529 -0.025930   220.0  \n",
       "441 -0.039493 -0.004222  0.003064    57.0  \n",
       "\n",
       "[442 rows x 11 columns]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_diabetes = load_diabetes(as_frame=True).frame\n",
    "df_diabetes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_diabetes.drop(columns={\"target\"})\n",
    "y = df_diabetes[\"target\"]\n",
    "\n",
    "X_train, X_test,  y_train,y_test = train_test_split(X, y)\n",
    "\n",
    "# stochastic_gradient_descent(training_data=X_train, y_train, \n",
    "#                             epochs=10, \n",
    "#                             mini_batch_size=10, \n",
    "#                             eta=0.1,\n",
    "#                             # test_data=[X_test, y_test] \n",
    "#                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Initialize network*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural net params:\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \n",
      "\n",
      "sizes : [10, 5, 2, 3, 1]\n",
      "\n",
      "n_layers : 5\n",
      "\n",
      "biases : [array([[-1.45338286],\n",
      "       [-0.39168747],\n",
      "       [ 1.13150022],\n",
      "       [-0.29559113],\n",
      "       [-0.08120438]]), array([[-0.27546487],\n",
      "       [-0.37319936]]), array([[ 1.28450169],\n",
      "       [ 1.14590927],\n",
      "       [-0.38055646]]), array([[-1.82799646]])]\n",
      "\n",
      "weights : [array([[ 0.10975792,  0.41738425,  0.64680674,  0.76950482, -0.20730376,\n",
      "         1.07701396,  1.02438534, -0.75169375, -0.97288412, -1.49591214],\n",
      "       [ 1.71088364,  0.30645547, -1.0723421 , -0.52383408, -2.36799358,\n",
      "        -0.78364916,  0.37082096, -0.62093282,  3.00825363,  0.14008442],\n",
      "       [ 1.09794203,  2.53548339,  0.71899945, -0.12590804, -0.54857506,\n",
      "         0.72166867, -0.0033914 ,  0.88302673,  0.01148581, -0.88678729],\n",
      "       [ 1.25867801,  0.53833315,  0.26906225,  0.61217412, -0.66015898,\n",
      "         0.5615016 , -0.70531924, -1.1447175 ,  0.45102603,  0.82192623],\n",
      "       [-1.31243973, -0.66680938,  0.91472688,  0.0780066 , -0.55953188,\n",
      "         0.75674621,  2.51269871,  1.8579995 , -2.08769382,  0.75111342]]), array([[ 0.55473916,  0.78941924, -0.87405024, -0.30179569, -0.22882991],\n",
      "       [ 0.81094639,  0.07941763,  0.25128596, -0.43246366, -0.09661666]]), array([[-0.74221308,  1.6006615 ],\n",
      "       [ 0.79308465,  1.25056507],\n",
      "       [-2.13229856, -0.50414735]]), array([[ 0.01775039,  0.26706133, -1.48639716]])]\n",
      "\n",
      "biases_shape : [(5, 1), (2, 1), (3, 1), (1, 1)]\n",
      "\n",
      "weights_shape : [(5, 10), (2, 5), (3, 2), (1, 3)]\n",
      "\n",
      "\n",
      " ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sizes = [10, 5, 2, 3, 1]\n",
    "initialize_network(network_size=sizes)\n",
    "network_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Feedforward*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(batch_size=10, early_stopping=True,\n",
       "              hidden_layer_sizes=[2, 4, 2, 5], learning_rate=0.05, max_iter=300,\n",
       "              solver=&#x27;sgd&#x27;, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link \">i<span>Not fitted</span></span></label><div class=\"sk-toggleable__content \"><pre>MLPClassifier(batch_size=10, early_stopping=True,\n",
       "              hidden_layer_sizes=[2, 4, 2, 5], learning_rate=0.05, max_iter=300,\n",
       "              solver=&#x27;sgd&#x27;, verbose=True)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(batch_size=10, early_stopping=True,\n",
       "              hidden_layer_sizes=[2, 4, 2, 5], learning_rate=0.05, max_iter=300,\n",
       "              solver='sgd', verbose=True)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier, MLPClassifier, BernoulliRBM\n",
    "\n",
    "model_perceptron = MLPClassifier(hidden_layer_sizes=[2,4,2,5],              # array-like represent for hidden layers size\n",
    "                                 solver='sgd',                              # solver = optimizer\n",
    "                                 activation='relu',                         # rectified linear unit\n",
    "                                 batch_size=10,                             # mini_batch size\n",
    "                                 learning_rate=0.05,                        # learning rate eta.\n",
    "                                 alpha=0.0001,                              # L2 regularization term, set by default\n",
    "                                 max_iter=300,                              # epochs\n",
    "                                 shuffle=True,                              # shuffle before split mini batch, use for 'sgd' or 'adam'\n",
    "                                 verbose=True,                              # display the training progress\n",
    "                                 early_stopping=True,                       # \n",
    "\n",
    "                                #  momentum=0.2                               # Momentum Stochastic Gradient Descent\n",
    "                                 )\n",
    "\n",
    "model_perceptron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:  2.17.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Tensorflow version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Tensorflow (Keras API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Breast cancer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
       "0                   0.07871  ...        25.380          17.33   \n",
       "1                   0.05667  ...        24.990          23.41   \n",
       "2                   0.05999  ...        23.570          25.53   \n",
       "3                   0.09744  ...        14.910          26.50   \n",
       "4                   0.05883  ...        22.540          16.67   \n",
       "..                      ...  ...           ...            ...   \n",
       "564                 0.05623  ...        25.450          26.40   \n",
       "565                 0.05533  ...        23.690          38.25   \n",
       "566                 0.05648  ...        18.980          34.12   \n",
       "567                 0.07016  ...        25.740          39.42   \n",
       "568                 0.05884  ...         9.456          30.37   \n",
       "\n",
       "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
       "0             184.60      2019.0           0.16220            0.66560   \n",
       "1             158.80      1956.0           0.12380            0.18660   \n",
       "2             152.50      1709.0           0.14440            0.42450   \n",
       "3              98.87       567.7           0.20980            0.86630   \n",
       "4             152.20      1575.0           0.13740            0.20500   \n",
       "..               ...         ...               ...                ...   \n",
       "564           166.10      2027.0           0.14100            0.21130   \n",
       "565           155.00      1731.0           0.11660            0.19220   \n",
       "566           126.70      1124.0           0.11390            0.30940   \n",
       "567           184.60      1821.0           0.16500            0.86810   \n",
       "568            59.16       268.6           0.08996            0.06444   \n",
       "\n",
       "     worst concavity  worst concave points  worst symmetry  \\\n",
       "0             0.7119                0.2654          0.4601   \n",
       "1             0.2416                0.1860          0.2750   \n",
       "2             0.4504                0.2430          0.3613   \n",
       "3             0.6869                0.2575          0.6638   \n",
       "4             0.4000                0.1625          0.2364   \n",
       "..               ...                   ...             ...   \n",
       "564           0.4107                0.2216          0.2060   \n",
       "565           0.3215                0.1628          0.2572   \n",
       "566           0.3403                0.1418          0.2218   \n",
       "567           0.9387                0.2650          0.4087   \n",
       "568           0.0000                0.0000          0.2871   \n",
       "\n",
       "     worst fractal dimension  \n",
       "0                    0.11890  \n",
       "1                    0.08902  \n",
       "2                    0.08758  \n",
       "3                    0.17300  \n",
       "4                    0.07678  \n",
       "..                       ...  \n",
       "564                  0.07115  \n",
       "565                  0.06637  \n",
       "566                  0.07820  \n",
       "567                  0.12400  \n",
       "568                  0.07039  \n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_cancer = load_breast_cancer(as_frame=True).frame\n",
    "df_cancer\n",
    "\n",
    "X, y = df_cancer, df_cancer.pop(\"target\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=16)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "\n",
    "model_nn = keras.models.Sequential()\n",
    "model_nn.add(keras.layers.Input(shape=[input_size]))\n",
    "\n",
    "model_nn.add(keras.layers.Dense(units=20, activation=keras.activations.relu))\n",
    "model_nn.add(keras.layers.Dense(units=10, activation=keras.activations.relu))\n",
    "model_nn.add(keras.layers.Dense(units=5, activation=keras.activations.relu))\n",
    "model_nn.add(keras.layers.Dense(units=3, activation=keras.activations.relu))\n",
    "model_nn.add(keras.layers.Dense(units=1, activation=keras.activations.relu))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_35\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_35\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_105 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">620</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_106 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">210</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_107 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_108 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_109 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_105 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │           \u001b[38;5;34m620\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_106 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m210\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_107 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │            \u001b[38;5;34m55\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_108 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m18\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_109 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m4\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">907</span> (3.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m907\u001b[0m (3.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">907</span> (3.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m907\u001b[0m (3.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_nn.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), \n",
    "              loss=keras.losses.binary_crossentropy,\n",
    "              metrics=[\"accuracy\", \"auc\"])\n",
    "\n",
    "model_nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 36ms/step - accuracy: 0.3843 - auc: 0.5000 - loss: 9.9238 - val_accuracy: 0.3636 - val_auc: 0.5000 - val_loss: 10.2570\n",
      "Epoch 2/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3934 - auc: 0.5000 - loss: 9.7776 - val_accuracy: 0.3636 - val_auc: 0.5000 - val_loss: 10.2570\n",
      "Epoch 3/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3848 - auc: 0.5000 - loss: 9.9153 - val_accuracy: 0.3636 - val_auc: 0.5000 - val_loss: 10.2570\n",
      "Epoch 4/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3547 - auc: 0.5000 - loss: 10.4004 - val_accuracy: 0.3636 - val_auc: 0.5000 - val_loss: 10.2570\n",
      "Epoch 5/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3938 - auc: 0.5000 - loss: 9.7715 - val_accuracy: 0.3636 - val_auc: 0.5000 - val_loss: 10.2570\n",
      "Epoch 6/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3790 - auc: 0.5000 - loss: 10.0099 - val_accuracy: 0.3636 - val_auc: 0.5000 - val_loss: 10.2570\n",
      "Epoch 7/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3890 - auc: 0.5000 - loss: 9.8486 - val_accuracy: 0.3636 - val_auc: 0.5000 - val_loss: 10.2570\n",
      "Epoch 8/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3552 - auc: 0.5000 - loss: 10.3936 - val_accuracy: 0.3636 - val_auc: 0.5000 - val_loss: 10.2570\n",
      "Epoch 9/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3684 - auc: 0.5000 - loss: 10.1801 - val_accuracy: 0.3636 - val_auc: 0.5000 - val_loss: 10.2570\n",
      "Epoch 10/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3788 - auc: 0.5000 - loss: 10.0126 - val_accuracy: 0.3636 - val_auc: 0.5000 - val_loss: 10.2570\n",
      "Epoch 11/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3700 - auc: 0.5000 - loss: 10.1542 - val_accuracy: 0.3636 - val_auc: 0.5000 - val_loss: 10.2570\n",
      "Epoch 12/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3526 - auc: 0.5000 - loss: 10.4352 - val_accuracy: 0.3636 - val_auc: 0.5000 - val_loss: 10.2570\n",
      "Epoch 13/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3554 - auc: 0.5000 - loss: 10.3891 - val_accuracy: 0.3636 - val_auc: 0.5000 - val_loss: 10.2570\n",
      "Epoch 14/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3515 - auc: 0.5000 - loss: 10.4530 - val_accuracy: 0.3636 - val_auc: 0.5000 - val_loss: 10.2570\n",
      "Epoch 15/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3790 - auc: 0.5000 - loss: 10.0091 - val_accuracy: 0.3636 - val_auc: 0.5000 - val_loss: 10.2570\n",
      "Epoch 16/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3885 - auc: 0.5000 - loss: 9.8566 - val_accuracy: 0.3636 - val_auc: 0.5000 - val_loss: 10.2570\n",
      "Epoch 17/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3723 - auc: 0.5000 - loss: 10.1177 - val_accuracy: 0.3636 - val_auc: 0.5000 - val_loss: 10.2570\n",
      "Epoch 18/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3781 - auc: 0.5000 - loss: 10.0237 - val_accuracy: 0.3636 - val_auc: 0.5000 - val_loss: 10.2570\n",
      "Epoch 19/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4051 - auc: 0.5000 - loss: 9.5892 - val_accuracy: 0.3636 - val_auc: 0.5000 - val_loss: 10.2570\n",
      "Epoch 20/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3358 - auc: 0.4886 - loss: 10.7064 - val_accuracy: 0.3636 - val_auc: 0.5000 - val_loss: 10.2570\n",
      "Epoch 21/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3775 - auc: 0.5000 - loss: 10.0331 - val_accuracy: 0.3636 - val_auc: 0.5000 - val_loss: 10.2570\n",
      "Epoch 22/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3581 - auc: 0.5000 - loss: 10.3464 - val_accuracy: 0.3636 - val_auc: 0.5000 - val_loss: 10.2570\n",
      "Epoch 23/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4080 - auc: 0.5000 - loss: 9.5416 - val_accuracy: 0.3636 - val_auc: 0.5000 - val_loss: 10.2570\n",
      "Epoch 24/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4037 - auc: 0.5000 - loss: 9.6110 - val_accuracy: 0.3636 - val_auc: 0.5000 - val_loss: 10.2570\n",
      "Epoch 25/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3836 - auc: 0.5000 - loss: 9.9355 - val_accuracy: 0.3636 - val_auc: 0.5000 - val_loss: 10.2570\n",
      "Epoch 26/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3342 - auc: 0.5000 - loss: 10.7321 - val_accuracy: 0.3636 - val_auc: 0.5000 - val_loss: 10.2570\n",
      "Epoch 27/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3857 - auc: 0.5000 - loss: 9.9012 - val_accuracy: 0.3636 - val_auc: 0.5000 - val_loss: 10.2570\n",
      "Epoch 28/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4225 - auc: 0.5000 - loss: 9.3080 - val_accuracy: 0.3636 - val_auc: 0.5000 - val_loss: 10.2570\n",
      "Epoch 29/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4049 - auc: 0.5000 - loss: 9.5922 - val_accuracy: 0.3636 - val_auc: 0.5000 - val_loss: 10.2570\n",
      "Epoch 30/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4072 - auc: 0.5000 - loss: 9.5553 - val_accuracy: 0.3636 - val_auc: 0.5000 - val_loss: 10.2570\n",
      "Epoch 31/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4081 - auc: 0.5000 - loss: 9.5404 - val_accuracy: 0.3636 - val_auc: 0.5000 - val_loss: 10.2570\n",
      "Epoch 31: early stopping\n"
     ]
    }
   ],
   "source": [
    "early = keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", \n",
    "                                    min_delta=0, \n",
    "                                    patience=30, \n",
    "                                    verbose=1, \n",
    "                                    mode=\"max\")\n",
    "\n",
    "history = model_nn.fit(x=X_train.values, \n",
    "                        y=y_train,\n",
    "                        epochs=100, \n",
    "                        batch_size=10, \n",
    "                        callbacks=early,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        verbose=True,\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(history.history).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version:  2.5.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Pytorch version: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
